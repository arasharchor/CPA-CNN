{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2283, 40, 40)\n"
     ]
    }
   ],
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img_rows = 40\n",
    "img_cols = 40\n",
    "nb_classes = 23\n",
    "\n",
    "# Generate X and y for Convolutional Neural Network\n",
    "X = []\n",
    "y= []\n",
    "\n",
    "# Get all the images and convert them into greyscale\n",
    "for path in glob.glob('./cropped_data/*.png'):\n",
    "    im = Image.open(path)\n",
    "    grey_im = im.convert('L')\n",
    "    # Convert them into numpy\n",
    "    X += [np.array(grey_im)]\n",
    "    y += [path.split('_')[1].split('/')[1]] # extract the label from path\n",
    "    \n",
    "X = np.array(X)\n",
    "print X.shape\n",
    "y = np.array(y)\n",
    "\n",
    "X_train = X # stupid, taking full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2283, 'train samples')\n"
     ]
    }
   ],
   "source": [
    "# Normalize \n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "#X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "X_train = X_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "X_train /= 255 #Normalize\n",
    "#X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "#print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15767625"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.mean() # maybe minus mean image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'halfcircle': 7, 'interphase': 10, 'metaphase': 15, 'earlyprophase': 4, 'debris': 3, 'nucleolirim': 19, 'kidney': 11, 'telophase': 22, 'blurry': 2, 'fragmented': 6, 'apoptotic': 1, 'latepro': 12, 'multinucleate': 18, 'prophase': 20, 'anaphase': 0, 'indented': 9, 'monopole': 17, 'holey': 8, 'latetelophase': 13, 'lines': 14, 'micronucleus': 16, 'round': 21, 'elongated': 5}\n",
      "(2283, 23)\n"
     ]
    }
   ],
   "source": [
    "# Convert y into Y\n",
    "\n",
    "from keras.utils import np_utils, generic_utils\n",
    "\n",
    "# Convert labels to numeric\n",
    "y_unique = np.unique(y)\n",
    "dic = {}\n",
    "\n",
    "for i, label in enumerate(y_unique):\n",
    "    dic[label] = i\n",
    "print dic\n",
    "\n",
    "y_numeric = []\n",
    "for el in y:\n",
    "    y_numeric += [dic[el]]\n",
    "    \n",
    "y_numeric # now a 2000 label vector\n",
    "Y = np_utils.to_categorical(y_numeric, nb_classes)\n",
    "\n",
    "print Y.shape\n",
    "\n",
    "Y_train = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Convolutional Neural Network with 2 convolutions\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "\n",
    "batch_size = 128\n",
    "nb_epoch = 12\n",
    "\n",
    "\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "# the data images are greyscale\n",
    "img_channels = 1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                        border_mode='valid',\n",
    "                        input_shape=(img_channels, img_rows, img_cols)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "2283/2283 [==============================] - 11s - loss: 2.5177 - acc: 0.4279    \n",
      "Epoch 2/12\n",
      "2283/2283 [==============================] - 11s - loss: 2.4881 - acc: 0.4284    \n",
      "Epoch 3/12\n",
      "2283/2283 [==============================] - 12s - loss: 2.4403 - acc: 0.4288    \n",
      "Epoch 4/12\n",
      "2283/2283 [==============================] - 12s - loss: 2.4161 - acc: 0.4314    \n",
      "Epoch 5/12\n",
      "2283/2283 [==============================] - 13s - loss: 2.3741 - acc: 0.4363    \n",
      "Epoch 6/12\n",
      "2283/2283 [==============================] - 13s - loss: 2.3504 - acc: 0.4402    \n",
      "Epoch 7/12\n",
      "2283/2283 [==============================] - 12s - loss: 2.3351 - acc: 0.4455    \n",
      "Epoch 8/12\n",
      "2283/2283 [==============================] - 12s - loss: 2.2998 - acc: 0.4437    \n",
      "Epoch 9/12\n",
      "2283/2283 [==============================] - 12s - loss: 2.2811 - acc: 0.4463    \n",
      "Epoch 10/12\n",
      "2283/2283 [==============================] - 12s - loss: 2.2343 - acc: 0.4551    \n",
      "Epoch 11/12\n",
      "2283/2283 [==============================] - 12s - loss: 2.2205 - acc: 0.4538    \n",
      "Epoch 12/12\n",
      "2283/2283 [==============================] - 12s - loss: 2.2063 - acc: 0.4604    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1125603d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "log = callbacks.TensorBoard(log_dir='./logs', histogram_freq=0)\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, callbacks=[log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2283/2283 [==============================] - 12s - loss: 2.1338 - acc: 0.4604    \n",
      "Epoch 2/200\n",
      "2283/2283 [==============================] - 12s - loss: 2.1193 - acc: 0.4652    \n",
      "Epoch 3/200\n",
      "2283/2283 [==============================] - 13s - loss: 2.0880 - acc: 0.4639    \n",
      "Epoch 4/200\n",
      "2283/2283 [==============================] - 14s - loss: 2.0906 - acc: 0.4621    \n",
      "Epoch 5/200\n",
      "2283/2283 [==============================] - 13s - loss: 2.0661 - acc: 0.4630    \n",
      "Epoch 6/200\n",
      "2283/2283 [==============================] - 14s - loss: 2.0551 - acc: 0.4674    \n",
      "Epoch 7/200\n",
      "2283/2283 [==============================] - 14s - loss: 2.0250 - acc: 0.4590    \n",
      "Epoch 8/200\n",
      "2283/2283 [==============================] - 13s - loss: 2.0023 - acc: 0.4691    \n",
      "Epoch 9/200\n",
      "2283/2283 [==============================] - 14s - loss: 2.0124 - acc: 0.4700    \n",
      "Epoch 10/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.9765 - acc: 0.4713    \n",
      "Epoch 11/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.9898 - acc: 0.4669    \n",
      "Epoch 12/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.9597 - acc: 0.4814    \n",
      "Epoch 13/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.9337 - acc: 0.4783    \n",
      "Epoch 14/200\n",
      "2283/2283 [==============================] - 16s - loss: 1.9330 - acc: 0.4836    \n",
      "Epoch 15/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.9349 - acc: 0.4696    \n",
      "Epoch 16/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.9457 - acc: 0.4901    \n",
      "Epoch 17/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.8964 - acc: 0.4849    \n",
      "Epoch 18/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.9109 - acc: 0.4871    \n",
      "Epoch 19/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.8930 - acc: 0.4858    \n",
      "Epoch 20/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.9019 - acc: 0.4901    \n",
      "Epoch 21/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.8865 - acc: 0.4858    \n",
      "Epoch 22/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.8646 - acc: 0.4910    \n",
      "Epoch 23/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.8592 - acc: 0.4941    \n",
      "Epoch 24/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.8521 - acc: 0.5007    \n",
      "Epoch 25/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.8575 - acc: 0.4985    \n",
      "Epoch 26/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.8320 - acc: 0.5077    \n",
      "Epoch 27/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.8670 - acc: 0.4871    \n",
      "Epoch 28/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.8290 - acc: 0.5024    \n",
      "Epoch 29/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.8097 - acc: 0.5050    \n",
      "Epoch 30/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.8186 - acc: 0.4954    \n",
      "Epoch 31/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.8074 - acc: 0.4998    \n",
      "Epoch 32/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.7903 - acc: 0.5077    \n",
      "Epoch 33/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.7938 - acc: 0.5164    \n",
      "Epoch 34/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.8005 - acc: 0.5107    \n",
      "Epoch 35/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.7917 - acc: 0.5059    \n",
      "Epoch 36/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.7803 - acc: 0.5085    \n",
      "Epoch 37/200\n",
      "2283/2283 [==============================] - 16s - loss: 1.7588 - acc: 0.5186    \n",
      "Epoch 38/200\n",
      "2283/2283 [==============================] - 16s - loss: 1.7477 - acc: 0.5204    \n",
      "Epoch 39/200\n",
      "2283/2283 [==============================] - 16s - loss: 1.7696 - acc: 0.5217    \n",
      "Epoch 40/200\n",
      "2283/2283 [==============================] - 20s - loss: 1.7763 - acc: 0.5287    \n",
      "Epoch 41/200\n",
      "2283/2283 [==============================] - 20s - loss: 1.7387 - acc: 0.5191    \n",
      "Epoch 42/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.7357 - acc: 0.5304    \n",
      "Epoch 43/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.7484 - acc: 0.5239    \n",
      "Epoch 44/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.7386 - acc: 0.5138    \n",
      "Epoch 45/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.7172 - acc: 0.5274    \n",
      "Epoch 46/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.7133 - acc: 0.5208    \n",
      "Epoch 47/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.7260 - acc: 0.5291    \n",
      "Epoch 48/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.7109 - acc: 0.5283    \n",
      "Epoch 49/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.7190 - acc: 0.5261    \n",
      "Epoch 50/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.7003 - acc: 0.5261    \n",
      "Epoch 51/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.6987 - acc: 0.5274    \n",
      "Epoch 52/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.6743 - acc: 0.5322    \n",
      "Epoch 53/200\n",
      "2283/2283 [==============================] - 19s - loss: 1.6989 - acc: 0.5274    \n",
      "Epoch 54/200\n",
      "2283/2283 [==============================] - 19s - loss: 1.7035 - acc: 0.5331    \n",
      "Epoch 55/200\n",
      "2283/2283 [==============================] - 19s - loss: 1.6716 - acc: 0.5278    \n",
      "Epoch 56/200\n",
      "2283/2283 [==============================] - 19s - loss: 1.6826 - acc: 0.5261    \n",
      "Epoch 57/200\n",
      "2283/2283 [==============================] - 19s - loss: 1.6837 - acc: 0.5287    \n",
      "Epoch 58/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.6895 - acc: 0.5313    \n",
      "Epoch 59/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.6718 - acc: 0.5309    \n",
      "Epoch 60/200\n",
      "2283/2283 [==============================] - 24s - loss: 1.6620 - acc: 0.5313    \n",
      "Epoch 61/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.6743 - acc: 0.5278    \n",
      "Epoch 62/200\n",
      "2283/2283 [==============================] - 24s - loss: 1.6655 - acc: 0.5313    \n",
      "Epoch 63/200\n",
      "2283/2283 [==============================] - 25s - loss: 1.6635 - acc: 0.5375    \n",
      "Epoch 64/200\n",
      "2283/2283 [==============================] - 24s - loss: 1.6452 - acc: 0.5339    \n",
      "Epoch 65/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.6634 - acc: 0.5300    \n",
      "Epoch 66/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.6701 - acc: 0.5335    \n",
      "Epoch 67/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.6459 - acc: 0.5353    \n",
      "Epoch 68/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.6166 - acc: 0.5502    \n",
      "Epoch 69/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.6257 - acc: 0.5401    \n",
      "Epoch 70/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.6215 - acc: 0.5396    \n",
      "Epoch 71/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.6293 - acc: 0.5348    \n",
      "Epoch 72/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.6209 - acc: 0.5445    \n",
      "Epoch 73/200\n",
      "2283/2283 [==============================] - 24s - loss: 1.6106 - acc: 0.5519    \n",
      "Epoch 74/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.5936 - acc: 0.5502    \n",
      "Epoch 75/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.6035 - acc: 0.5366    \n",
      "Epoch 76/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.5953 - acc: 0.5471    \n",
      "Epoch 77/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.5962 - acc: 0.5532    \n",
      "Epoch 78/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.5995 - acc: 0.5497    \n",
      "Epoch 79/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.5967 - acc: 0.5484    \n",
      "Epoch 80/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.5987 - acc: 0.5523    \n",
      "Epoch 81/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.5820 - acc: 0.5488    \n",
      "Epoch 82/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.6069 - acc: 0.5466    \n",
      "Epoch 83/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.5774 - acc: 0.5537    \n",
      "Epoch 84/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.5792 - acc: 0.5515    \n",
      "Epoch 85/200\n",
      "2283/2283 [==============================] - 24s - loss: 1.5668 - acc: 0.5541    \n",
      "Epoch 86/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.5854 - acc: 0.5388    \n",
      "Epoch 87/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.5821 - acc: 0.5458    \n",
      "Epoch 88/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.5717 - acc: 0.5541    \n",
      "Epoch 89/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.5639 - acc: 0.5598    \n",
      "Epoch 90/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.5653 - acc: 0.5567    \n",
      "Epoch 91/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.5703 - acc: 0.5458    \n",
      "Epoch 92/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.5648 - acc: 0.5594    \n",
      "Epoch 93/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.5644 - acc: 0.5471    \n",
      "Epoch 94/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.5650 - acc: 0.5497    \n",
      "Epoch 95/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.5504 - acc: 0.5580    \n",
      "Epoch 96/200\n",
      "2283/2283 [==============================] - 24s - loss: 1.5379 - acc: 0.5598    \n",
      "Epoch 97/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.5424 - acc: 0.5558    \n",
      "Epoch 98/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.5589 - acc: 0.5646    \n",
      "Epoch 99/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.5446 - acc: 0.5580    \n",
      "Epoch 100/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.5250 - acc: 0.5563    \n",
      "Epoch 101/200\n",
      "2283/2283 [==============================] - 20s - loss: 1.5424 - acc: 0.5558    \n",
      "Epoch 102/200\n",
      "2283/2283 [==============================] - 23s - loss: 1.5379 - acc: 0.5558    \n",
      "Epoch 103/200\n",
      "2283/2283 [==============================] - 20s - loss: 1.5264 - acc: 0.5624    \n",
      "Epoch 104/200\n",
      "2283/2283 [==============================] - 20s - loss: 1.5238 - acc: 0.5550    \n",
      "Epoch 105/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.5277 - acc: 0.5602    \n",
      "Epoch 106/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.5153 - acc: 0.5624    \n",
      "Epoch 107/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.5303 - acc: 0.5637    \n",
      "Epoch 108/200\n",
      "2283/2283 [==============================] - 20s - loss: 1.5228 - acc: 0.5563    \n",
      "Epoch 109/200\n",
      "2283/2283 [==============================] - 17s - loss: 1.4961 - acc: 0.5642    \n",
      "Epoch 110/200\n",
      "2283/2283 [==============================] - 18s - loss: 1.5117 - acc: 0.5585    \n",
      "Epoch 111/200\n",
      "2283/2283 [==============================] - 16s - loss: 1.4944 - acc: 0.5738    \n",
      "Epoch 112/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4984 - acc: 0.5629    \n",
      "Epoch 113/200\n",
      "2283/2283 [==============================] - 16s - loss: 1.4984 - acc: 0.5681    \n",
      "Epoch 114/200\n",
      "2283/2283 [==============================] - 16s - loss: 1.5038 - acc: 0.5690    \n",
      "Epoch 115/200\n",
      "2283/2283 [==============================] - 18s - loss: 1.5223 - acc: 0.5611    \n",
      "Epoch 116/200\n",
      "2283/2283 [==============================] - 18s - loss: 1.5014 - acc: 0.5629    \n",
      "Epoch 117/200\n",
      "2283/2283 [==============================] - 18s - loss: 1.5040 - acc: 0.5672    \n",
      "Epoch 118/200\n",
      "2283/2283 [==============================] - 18s - loss: 1.4907 - acc: 0.5690    \n",
      "Epoch 119/200\n",
      "2283/2283 [==============================] - 18s - loss: 1.5041 - acc: 0.5624    \n",
      "Epoch 120/200\n",
      "2283/2283 [==============================] - 19s - loss: 1.5042 - acc: 0.5672    \n",
      "Epoch 121/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.4938 - acc: 0.5637    \n",
      "Epoch 122/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4725 - acc: 0.5668    \n",
      "Epoch 123/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.4830 - acc: 0.5624    \n",
      "Epoch 124/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.5001 - acc: 0.5602    \n",
      "Epoch 125/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.4490 - acc: 0.5813    \n",
      "Epoch 126/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.4896 - acc: 0.5694    \n",
      "Epoch 127/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4922 - acc: 0.5721    \n",
      "Epoch 128/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4632 - acc: 0.5703    \n",
      "Epoch 129/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4773 - acc: 0.5672    \n",
      "Epoch 130/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4579 - acc: 0.5848    \n",
      "Epoch 131/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4647 - acc: 0.5734    \n",
      "Epoch 132/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4601 - acc: 0.5834    \n",
      "Epoch 133/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4877 - acc: 0.5786    \n",
      "Epoch 134/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4656 - acc: 0.5742    \n",
      "Epoch 135/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4784 - acc: 0.5738    \n",
      "Epoch 136/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4663 - acc: 0.5725    \n",
      "Epoch 137/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4740 - acc: 0.5729    \n",
      "Epoch 138/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.4527 - acc: 0.5734    \n",
      "Epoch 139/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4651 - acc: 0.5782    \n",
      "Epoch 140/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4519 - acc: 0.5786    \n",
      "Epoch 141/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4372 - acc: 0.5729    \n",
      "Epoch 142/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4385 - acc: 0.5786    \n",
      "Epoch 143/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4535 - acc: 0.5716    \n",
      "Epoch 144/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4526 - acc: 0.5821    \n",
      "Epoch 145/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4545 - acc: 0.5756    \n",
      "Epoch 146/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.4482 - acc: 0.5729    \n",
      "Epoch 147/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4494 - acc: 0.5834    \n",
      "Epoch 148/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.4327 - acc: 0.5703    \n",
      "Epoch 149/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.4313 - acc: 0.5834    \n",
      "Epoch 150/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4428 - acc: 0.5699    \n",
      "Epoch 151/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4252 - acc: 0.5865    \n",
      "Epoch 152/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4208 - acc: 0.5769    \n",
      "Epoch 153/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4256 - acc: 0.5830    \n",
      "Epoch 154/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.4279 - acc: 0.5830    \n",
      "Epoch 155/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4373 - acc: 0.5773    \n",
      "Epoch 156/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.4167 - acc: 0.5773    \n",
      "Epoch 157/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4246 - acc: 0.5817    \n",
      "Epoch 158/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.3971 - acc: 0.5887    \n",
      "Epoch 159/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.4286 - acc: 0.5878    \n",
      "Epoch 160/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4104 - acc: 0.5891    \n",
      "Epoch 161/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4191 - acc: 0.5869    \n",
      "Epoch 162/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.4053 - acc: 0.5852    \n",
      "Epoch 163/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.3981 - acc: 0.5826    \n",
      "Epoch 164/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.4058 - acc: 0.5830    \n",
      "Epoch 165/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.3920 - acc: 0.5909    \n",
      "Epoch 166/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.4079 - acc: 0.5983    \n",
      "Epoch 167/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.4176 - acc: 0.5865    \n",
      "Epoch 168/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.3993 - acc: 0.5935    \n",
      "Epoch 169/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.3668 - acc: 0.5966    \n",
      "Epoch 170/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.3987 - acc: 0.5769    \n",
      "Epoch 171/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.4086 - acc: 0.5900    \n",
      "Epoch 172/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.4022 - acc: 0.5883    \n",
      "Epoch 173/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.3877 - acc: 0.5961    \n",
      "Epoch 174/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.3868 - acc: 0.5883    \n",
      "Epoch 175/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.3896 - acc: 0.5922    \n",
      "Epoch 176/200\n",
      "2283/2283 [==============================] - 16s - loss: 1.3957 - acc: 0.5948    \n",
      "Epoch 177/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.3778 - acc: 0.5961    \n",
      "Epoch 178/200\n",
      "2283/2283 [==============================] - 15s - loss: 1.3697 - acc: 0.5896    \n",
      "Epoch 179/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.3969 - acc: 0.5852    \n",
      "Epoch 180/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.4021 - acc: 0.5861    \n",
      "Epoch 181/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.3554 - acc: 0.5935    \n",
      "Epoch 182/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.3737 - acc: 0.6036    \n",
      "Epoch 183/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.3680 - acc: 0.6062    \n",
      "Epoch 184/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.3778 - acc: 0.6045    \n",
      "Epoch 185/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.3623 - acc: 0.5931    \n",
      "Epoch 186/200\n",
      "2283/2283 [==============================] - 16s - loss: 1.3696 - acc: 0.5922    \n",
      "Epoch 187/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.3846 - acc: 0.5926    \n",
      "Epoch 188/200\n",
      "2283/2283 [==============================] - 22s - loss: 1.3569 - acc: 0.5988    \n",
      "Epoch 189/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.3559 - acc: 0.5896    \n",
      "Epoch 190/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.3697 - acc: 0.6014    \n",
      "Epoch 191/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.3692 - acc: 0.5922    \n",
      "Epoch 192/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.3531 - acc: 0.5996    \n",
      "Epoch 193/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.3566 - acc: 0.5961    \n",
      "Epoch 194/200\n",
      "2283/2283 [==============================] - 21s - loss: 1.3595 - acc: 0.5988    \n",
      "Epoch 195/200\n",
      "2283/2283 [==============================] - 16s - loss: 1.3753 - acc: 0.5948    \n",
      "Epoch 196/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.3393 - acc: 0.6071    \n",
      "Epoch 197/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.3307 - acc: 0.5988    \n",
      "Epoch 198/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.3621 - acc: 0.5983    \n",
      "Epoch 199/200\n",
      "2283/2283 [==============================] - 13s - loss: 1.3208 - acc: 0.6124    \n",
      "Epoch 200/200\n",
      "2283/2283 [==============================] - 14s - loss: 1.3313 - acc: 0.6027    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11bcfae90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, callbacks=[log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "remote = callbacks.RemoteMonitor(root='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "2283/2283 [==============================] - 14s - loss: 2.7943 - acc: 0.3710    \n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named requests",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ab1360614294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Python/2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, show_accuracy, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m    643\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                          \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                          shuffle=shuffle, metrics=metrics)\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, metrics)\u001b[0m\n\u001b[1;32m    299\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0msend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0msend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named requests"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, callbacks=[remote])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
